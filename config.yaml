# HOG/LBP/SVM Pipeline Configuration
# ===================================
# This file contains all configurable parameters for the nurdle detection pipeline.
# Modify these values to customize the behavior without changing code.

# Project Information
project:
  name: "Nurdle Detection Pipeline"
  version: "1.0.0"
  description: "HOG/LBP feature extraction with SVM classification for nurdle detection"
  
# File Paths and Directories
paths:
  # Input directory containing raw images and JSON annotation files
  input_dir: "input"
  
  # Output directory for all generated files
  output_dir: "output"
  
  # Subdirectories under output_dir
  models_dir: "models"
  evaluations_dir: "evaluations" 
  visualizations_dir: "visualizations"
  logs_dir: "logs"
  
  # Temporary directories for intermediate processing
  temp_dir: "temp"
  normalized_images_dir: "temp/normalized"
  candidate_windows_dir: "temp/candidate_windows"
  extracted_features_dir: "temp/extracted_features"
  
  # Test set directory (optional - if exists, will be used instead of train/test split)
  test_set_dir: "test_set"

# Data Processing Configuration
data:
  # Train/test split configuration
  test_size: 0.3  # Fraction of data to use for testing (0.0-1.0)
  random_state: 42  # Random seed for reproducible splits
  stratify: true  # Whether to maintain class balance in train/test split
  
  # Data validation settings
  min_images_required: 10  # Minimum number of images needed to run pipeline
  validate_json_format: true  # Whether to validate JSON annotation format
  
  # Image format requirements
  supported_formats: [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]
  max_image_size_mb: 50  # Maximum allowed image file size in MB

# Image Preprocessing Configuration
preprocessing:
  # Target window size for all processing (width, height)
  target_size: [40, 40]
  
  # Image normalization settings
  normalization:
    target_max_dimension: 1080  # Maximum dimension for normalized images
    preserve_aspect_ratio: true  # Maintain aspect ratios during resize
  
  # Contrast enhancement method
  # Options: "contrast_stretch", "histogram_eq", "adaptive_eq", "none"
  contrast_method: "contrast_stretch"
  
  # Parameters for contrast stretching (percentile clipping)
  contrast_percentiles: [2, 98]  # [lower_percentile, upper_percentile]
  
  # Parameters for adaptive histogram equalization (CLAHE)
  clahe:
    clip_limit: 2.0  # Contrast limiting threshold
    tile_grid_size: [2, 2]  # Size of neighborhood for local histogram calculation
  
  # Sliding window configuration for candidate generation
  sliding_window:
    window_size: [40, 40]  # Size of sliding window
    stride: 20  # Step size between windows (smaller = more overlap)
    
    # Separate stride for training vs inference (Phase 3.2 optimization)
    stride_train: 30  # Larger stride for training = fewer windows, faster
    stride_inference: 15  # Smaller stride for inference = better coverage
    use_separate_strides: false  # Enable to use stride_train/stride_inference
    
    # Boundary handling
    pad_image: true  # Whether to pad image to ensure full coverage
    min_window_coverage: 0.8  # Minimum fraction of window that must be inside image

# Feature Extraction Configuration
features:
  # Histogram of Oriented Gradients (HOG) parameters
  hog:
    orientations: 9  # Number of orientation bins
    pixels_per_cell: [8, 8]  # Size of cells in pixels (width, height)
    cells_per_block: [2, 2]  # Size of blocks in cells (width, height)
    block_norm: "L2-Hys"  # Block normalization method
    transform_sqrt: false  # Whether to apply power law compression
    
    # Color feature extraction (Phase 1.2)
    use_color: true  # Extract from RGB channels vs grayscale
    color_space: "RGB"  # "RGB", "HSV", "LAB" (for future experimentation)
    
  # Local Binary Pattern (LBP) parameters
  lbp:
    n_points: 8  # Number of sample points on circle
    radius: 1  # Radius of circle
    method: "uniform"  # LBP method: "uniform", "nri_uniform", "default"
    
  # Feature combination settings
  combination:
    # Which features to extract: "hog", "lbp", "combined"
    feature_types: ["hog", "lbp", "combined"]
    
    # Whether to normalize features before combining
    normalize_before_combine: false
  
  # Feature dimensions (for reference - auto-calculated)
  # Grayscale HOG: ~81 dimensions
  # RGB HOG: ~243 dimensions (81 Ã— 3 channels)
  # LBP: 10 dimensions (grayscale)
  # Combined grayscale: 91 dimensions
  # Combined RGB: 253 dimensions
  
  # Dimensionality Reduction (Phase 3.3)
  dimensionality_reduction:
    enabled: false  # Enable PCA to reduce feature dimensions
    method: "pca"  # Only "pca" supported currently
    
    # PCA configuration
    pca:
      n_components: "auto"  # "auto", integer, or float (variance ratio)
      variance_threshold: 0.95  # If "auto", retain 95% of variance
      whiten: false  # Whether to whiten (normalize) PCA components
      
    # Manual target dimensions (optional - overrides n_components)
    target_dimensions:
      hog: 50  # Reduce HOG from 81/243 to 50 dimensions
      lbp: null  # null = keep original (LBP already small)
      combined: 80  # Target for combined features

# Model Training Configuration
training:
  # Window labeling method (Phase 1.1)
  labeling_method: "center_point"  # "center_point" or "iou" (legacy)
  
  # Center-point labeling configuration (Phase 1.1)
  center_point:
    allow_multiple_centers: false  # If >1 center in window, still label positive?
    center_tolerance_px: 0  # Pixels of tolerance around exact center
  
  # Legacy IoU threshold for positive window labeling (used if labeling_method="iou")
  iou_threshold: 0.5
  
  # Ground truth bounding box normalization size
  gt_bbox_size: 40
  
  # Hard Negative Mining (Phase 1.3)
  hard_negative_mining:
    enabled: true  # Enable two-stage training with hard negative mining
    
    # Stage 1: Initial training
    initial_negative_ratio: 3.0  # Negatives per positive in initial training
    initial_sample_method: "random"  # "random" or "stratified"
    
    # Stage 2: Hard negative collection
    false_positive_threshold: 0.0  # SVM decision threshold for FP collection
    max_hard_negatives: 10000  # Maximum hard negatives to collect
    hard_negative_ratio: 2.0  # Hard negatives per positive in final training
    
    # Iterations (for future enhancement)
    mining_iterations: 1  # Number of hard negative mining iterations
  
  # Bounding Box Regression (Phase 2.1)
  bounding_box_regression:
    enabled: false  # Enable bbox refinement after classification
    
    # Regression model type
    regressor_type: "svr"  # "svr" (Support Vector Regression) or "ridge" (Ridge Regression)
    
    # SVR parameters (used if regressor_type="svr")
    kernel: "linear"  # "linear", "rbf", "poly"
    C: 1.0  # Regularization parameter
    epsilon: 0.1  # Epsilon-tube for SVR
    
    # Ridge parameters (used if regressor_type="ridge")
    alpha: 1.0  # L2 regularization strength
    
    # Target normalization
    normalize_targets: true  # Normalize regression targets (recommended)
    clip_targets: true  # Clip targets to valid ranges
    max_offset_ratio: 0.5  # Maximum offset as fraction of window size
    
    # Multi-output handling
    independent_outputs: true  # Train separate regressor for each output dimension
  
  # Support Vector Machine (SVM) parameters
  svm:
    # SVM type and kernel
    kernel: "linear"  # "linear", "poly", "rbf", "sigmoid"
    C: 1.0  # Regularization parameter
    gamma: "scale"  # Kernel coefficient for rbf/poly/sigmoid
    
    # Training parameters
    max_iter: 1000  # Maximum number of iterations
    random_state: 42  # Random seed for reproducible results
    
    # Class balancing
    class_weight: null  # "balanced" for automatic balancing, null for no balancing
    
  # Cross-validation settings
  cross_validation:
    enabled: true
    folds: 5  # Number of CV folds
    scoring: "f1"  # Scoring metric: "accuracy", "precision", "recall", "f1"
    
  # Feature scaling
  scaling:
    method: "standard"  # "standard", "minmax", "robust", "none"
    
  # Model selection and hyperparameter tuning
  hyperparameter_tuning:
    enabled: true  # Whether to perform hyperparameter optimization
    use_optuna: true  # Use Optuna for optimization (if available)
    n_trials: 50  # Number of Optuna trials
    cv_folds: 3  # Folds for hyperparameter CV
    
    # Fallback GridSearch parameters (used if Optuna not available)
    param_grid:
      C: [0.1, 1.0, 10.0, 100.0]
      kernel: ["linear", "rbf"]
      gamma: ["scale", "auto"]
    
  # Ensemble methods
  ensemble:
    # Stacking classifier configuration
    stacking:
      enabled: true  # Whether to train stacking ensemble
      base_models: ["hog", "lbp"]  # Feature types for base models
      meta_classifier: "logistic_regression"  # "logistic_regression" or "svm"
      cv_folds: 3  # Folds for stacking CV

# Evaluation Configuration  
evaluation:
  # Test set configuration
  test_split_ratio: 0.2  # Portion of data reserved for testing
  
  # Metrics configuration
  metrics:
    calculate_mape: true  # Mean Absolute Percentage Error
    calculate_mae: true   # Mean Absolute Error
    # Note: Pipeline automatically works with real images and their JSON annotations
    # Fallback simulation only used if window metadata is missing
    
  # Visualization settings
  visualization:
    enabled: true  # Whether to generate visualizations
    
    # Plot styling
    figure_size: [12, 8]  # Default figure size (width, height)
    dpi: 300  # Image resolution for saved plots
    
  # Detection parameters
  detection:
    threshold: 0.0  # Default SVM decision boundary (will be optimized)
    iou_threshold: 0.5  # IoU threshold for Non-Maximum Suppression
    optimize_threshold: true  # Whether to optimize detection threshold
    threshold_optimization_metric: "f1"  # Metric for threshold optimization

# Multi-Scale Detection Configuration (Phase 2.2)
multi_scale_detection:
  # Image pyramid parameters
  pyramid:
    enabled: false  # Enable multi-scale detection via image pyramids
    
    # Pyramid generation
    scale_factor: 1.5  # Downsampling factor between levels (e.g., 1.5 = 67% of previous)
    min_size: [40, 40]  # Minimum image size (width, height) for pyramid levels
    max_levels: 5  # Maximum number of pyramid levels
    
    # Interpolation method for resizing
    # Options: "nearest", "linear", "cubic", "area", "lanczos"
    interpolation: "linear"
  
  # Scale-aware NMS parameters
  nms:
    enabled: false  # Use scale-aware NMS instead of standard NMS
    iou_threshold: 0.5  # IoU threshold for cross-scale suppression
    scale_penalty: 0.1  # Score penalty for coarser scales (0.0-1.0)
    # Higher penalty favors detections from finer (original-size) scales

# System Configuration
system:
  # Logging configuration
  logging:
    level: "INFO"  # "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file_logging: true  # Whether to log to file
    console_logging: true  # Whether to log to console
    
  # Performance and resource settings
  performance:
    # Parallel processing
    n_jobs: -1  # Number of parallel jobs (-1 for all cores, 1 for no parallelism)
    
    # Memory management (Phase 3.1)
    batch_processing:
      enabled: false  # Use generators for memory efficiency (large datasets)
      batch_size: 1000  # Windows per batch
      
    # Incremental training (Phase 3.1)
    incremental_training:
      enabled: false  # Use SGDClassifier instead of LinearSVC
      partial_fit_batch_size: 5000  # Samples per partial_fit call
      n_epochs: 5  # Number of passes over data
      learning_rate: "optimal"  # "constant", "optimal", "invscaling", "adaptive"
      
    # Feature caching
    cache_features_to_disk: false  # Save features to disk for large datasets
    feature_cache_dir: "temp/feature_cache"
    
    # Memory limits
    batch_size: 1000  # Batch size for processing large datasets
    memory_limit_gb: 8  # Memory limit in GB (for monitoring)
    
    # Progress reporting
    show_progress: true  # Whether to show progress bars
    progress_update_interval: 100  # Update progress every N items
    
  # Reproducibility
  reproducibility:
    set_random_seeds: true  # Whether to set random seeds for reproducibility
    random_seed: 42  # Master random seed
    
  # Development and debugging
  debug:
    save_intermediate_results: false  # Save intermediate processing results
    verbose_logging: false  # Enable verbose logging for debugging
    profile_performance: false  # Enable performance profiling

# Model Management
model_management:
  # Model saving and loading
  save_models: true  # Whether to save trained models
  model_format: "joblib"  # "joblib" or "pickle"
  
  # Model versioning
  versioning:
    enabled: true
    include_timestamp: true  # Include timestamp in model filenames
    include_config_hash: false  # Include config hash for reproducibility
    
  # Model metadata
  save_metadata: true  # Save model training metadata
  metadata_format: "json"  # Format for metadata files

# Pipeline Control
pipeline:
  # Which steps to run (useful for partial execution)
  steps:
    normalization: true
    train_test_split: true
    sliding_window_processing: true
    feature_extraction: true
    model_training: true
    evaluation: true
    
  # Error handling
  error_handling:
    continue_on_error: false  # Whether to continue pipeline if a step fails
    save_error_logs: true  # Save detailed error information
    
  # Cleanup
  cleanup:
    remove_temp_files: true  # Clean up temporary files after completion
    keep_intermediate_results: false  # Keep intermediate results for inspection