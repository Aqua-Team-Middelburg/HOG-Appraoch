# Nurdle Detection Pipeline - Two-Stage Detection Configuration
# ==============================================================
# SVM Classifier → SVR Regressor → Optional Stacking
# Optimized for simplicity and correctness

# Project Information
project:
  name: "Nurdle Detection Pipeline"
  version: "2.0.0"
  description: "Two-stage nurdle detection with SVM classifier and SVR coordinate regression"

# Data Processing Configuration
data:
  train_test_ratio: 0.8           # Fraction for training (0.8 = 80% train, 20% test)
  target_resolution: 1080         # Target image resolution for normalization
  target_nurdle_radius: null      # Target nurdle radius in pixels after normalization
                                  # Images will be scaled so nurdles have this approximate radius
                                  # Set to null to disable radius-based scaling (use resolution only)
                                  # Example: 20 means scale images so average nurdle has ~20px radius
  batch_size: 15                  # Images per RAM batch (memory management)
  input_dir: "input"              # Input directory with images and JSON files
  output_dir: "output"            # Output directory for results

# Window Processing Configuration
windows:
  size: [20, 20]                  # Window size [width, height]
  stride: 15                      # Sliding window stride

# Feature Extraction Configuration  
features:
  hog_cell_size: [4, 4]           # HOG cell size

# Training Configuration
training:
  negative_positive_ratio: 3.0    # Ratio of negative to positive samples (3:1)
  min_distance_from_nurdles: 25   # Minimum pixel distance for negative sample selection
  nurdle_radius: 10               # Expected nurdle radius in pixels (for window labeling)
                                  # A window is positive if its center is within this radius
                                  # of any nurdle center point

# Stacked Model Configuration (optional advanced mode)
stacking:
  enabled: false                  # Set to true to train and evaluate stacked model
  meta_learner: "logistic_regression"  # or "random_forest"

# Evaluation Configuration  
evaluation:
  # Test set configuration
  test_split_ratio: 0.2  # Portion of data reserved for testing
  
  # NMS Configuration
  nms:
    distance_threshold: 20  # Distance threshold in pixels for center-based NMS (default: 20 pixels, consider as multiplier of expected nurdle radius)
    # NMS Strategy: Offset-magnitude-based
    # Among nearby detections, keeps the one with smallest SVR offset magnitude
    # (most centered on the actual nurdle). This leverages SVR predictions as
    # a natural confidence metric without requiring separate confidence scoring.
  
  # Per-model evaluation
  per_model: true  # Enable separate evaluation for each model type
  
  # Metrics configuration
  metrics:
    calculate_mape: true  # Mean Absolute Percentage Error
    calculate_mae: true   # Mean Absolute Error
    # Note: Pipeline automatically works with real images and their JSON annotations
    # Fallback simulation only used if window metadata is missing
    
  # Visualization settings
  visualization:
    enabled: true  # Whether to generate visualizations
    
    # Plot styling
    figure_size: [12, 8]  # Default figure size (width, height)
    dpi: 300  # Image resolution for saved plots
    
  # Detection parameters
  detection:
    threshold: 0.0  # Default SVM decision boundary (will be optimized)
    iou_threshold: 0.5  # IoU threshold for Non-Maximum Suppression
    optimize_threshold: true  # Whether to optimize detection threshold
    threshold_optimization_metric: "f1"  # Metric for threshold optimization

# System Configuration
system:
  # Logging configuration
  logging:
    level: "INFO"  # "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file_logging: true  # Whether to log to file
    console_logging: true  # Whether to log to console
    
  # Performance and resource settings
  performance:
    # Parallel processing
    n_jobs: -1  # Number of parallel jobs (-1 for all cores, 1 for no parallelism)
    batch_size: 1000  # Batch size for processing large datasets
    
  # Reproducibility
  reproducibility:
    set_random_seeds: true  # Whether to set random seeds for reproducibility
    random_seed: 42  # Master random seed
    
  # Development and debugging
  debug:
    save_intermediate_results: false  # Save intermediate processing results
    verbose_logging: false  # Enable verbose logging for debugging
    profile_performance: false  # Enable performance profiling

# Model Management
model_management:
  # Model saving and loading
  save_models: true  # Whether to save trained models
  model_format: "joblib"  # "joblib" or "pickle"
  
  # Model versioning
  versioning:
    enabled: true
    include_timestamp: true  # Include timestamp in model filenames
    include_config_hash: false  # Include config hash for reproducibility
    
  # Model metadata
  save_metadata: true  # Save model training metadata
  metadata_format: "json"  # Format for metadata files

# Hyperparameter Optimization Configuration
optimization:
  # Sequential optimization strategy
  svm_optimization:
    metric: "f1_score"
    n_trials: 10
    parameter_ranges:
      svm_c: [0.001, 100.0]  # log scale
      svm_kernel: ["linear", "rbf"]
      svm_gamma: [0.001, 1.0]  # log scale, only for rbf
  
  svr_optimization:
    metric: "avg_coordinate_error"  # minimize this
    n_trials: 10
    parameter_ranges:
      svr_c: [0.001, 100.0]  # log scale
      svr_kernel: ["linear", "rbf"]
      svr_gamma: [0.001, 1.0]  # log scale, only for rbf
      svr_epsilon: [0.01, 1.0]  # regression tolerance
    
# Pipeline Control
pipeline:
  # Default steps to run (can be overridden via --steps argument)
  steps:
    - normalization
    - windows
    - features
    - training
    - evaluation
  
  # Checkpoints configuration
  checkpoints:
    enabled: true  # Enable checkpoint save/load
    
  # Error handling
  error_handling:
    continue_on_error: false  # Whether to continue pipeline if a step fails
    save_error_logs: true  # Save detailed error information
    
  # Cleanup
  cleanup:
    remove_temp_files: false  # Clean up temporary files after completion (set to false to keep checkpoints)
    keep_intermediate_results: true  # Keep intermediate results for inspection

# Visualization Configuration
visualization:
  samples_per_stage: 3  # Number of sample visualizations per stage